{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a571685b-6479-487f-90f0-f9ee0a756143",
   "metadata": {},
   "source": [
    "# For Sample\n",
    "[Convolutional Network Visualizations & Deep Dream](https://www.kaggle.com/code/carloalbertobarbano/convolutional-network-visualizations-deep-dream/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e80ea86d-2246-460c-9a18-6629d69b2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torchvision import models, transforms\n",
    "import PIL\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy.ndimage as nd\n",
    "import PIL.Image\n",
    "from IPython.display import clear_output, Image, display\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ad6209e4-1ce9-4202-9faa-d75c82fba5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_DEFINE = [0.485, 0.456, 0.406]\n",
    "STD_DEFINE = [0.229, 0.224, 0.225]\n",
    "IMAGE_RESIZE = (224, 224)\n",
    "\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12fad6b-22bf-4a3a-8908-137a69110d81",
   "metadata": {},
   "source": [
    "# np.uint8(np.clip(image, 0, 255))\n",
    "* 0 ~ 255 の値に収め、uint8のnp型にキャストする\n",
    "\n",
    "# バイト列を格納するBufferを用意する\n",
    "* buffer = BytesIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e8b0d4c-9b3d-443d-871d-c8eaf371cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_array(image, format='jpg'):\n",
    "    clip_np_image = np.uint8(np.clip(image, 0, 255))\n",
    "    buffer = BytesIO()\n",
    "    \n",
    "    PIL.Image.fromarray(clip_np_image).save(buffer, format)\n",
    "    display(Image(data=buffer.getvalue()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11692e-1dad-4c13-926b-76629067ffa5",
   "metadata": {},
   "source": [
    "# reshapeは配列を形状変換する\n",
    "* mean(平均)\n",
    "    * [0.485 0.456 0.406] -> [[[0.485 0.456 0.406]]]\n",
    "* std(標準偏差)\n",
    "    * [0.229, 0.224, 0.225] -> [[[0.229, 0.224, 0.225]]]\n",
    "* inp(インプット)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "922b2d09-2e19-44ce-b99b-aa87a523d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor(image):\n",
    "    mean = np.array(MEAN_DEFINE).reshape([1, 1, 3])\n",
    "    std = np.array(STD_DEFINE).reshape([1, 1, 3])\n",
    "\n",
    "    # インプット\n",
    "    inp = image[0, :, :, :]\n",
    "    inp = inp.transpose(1, 2, 0)\n",
    "    inp = std * inp + mean\n",
    "    inp *= 255\n",
    "\n",
    "    show_array(inp)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dcafe9f7-6d30-400d-8642-8ae834d58049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imaages(image, title=None):\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    \n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(10 / 5 + 1, 5, i + 1)\n",
    "        plt.axis('off')\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11f3f4-909c-47e0-bf87-2496e8f8ac26",
   "metadata": {},
   "source": [
    "# Resize\n",
    "* リサイズを行うTransform\n",
    "# ToTensor\n",
    "* PIL Image をテンソルに変換する Transform\n",
    "* 値の範囲は [0, 1] の float にスケールされる\n",
    "* 形状は (C, H, W) になる\n",
    "# Normalize\n",
    "* 正規化を行う Transform \n",
    "* n 個のチャンネルごとの平均 (m1,m2,⋯ ,mn),及び標準偏差 (s1,s2,⋯ ,sn) が与えられたとき、チャンネルごとに次のように標準化を行います。\n",
    "$$ \n",
    "    output_c = \\frac{input_c – m_c}s_c\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e22473f8-e04a-47a1-85a6-49ae86b178b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN_DEFINE, STD_DEFINE)\n",
    "])\n",
    "\n",
    "normalise_resize = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_RESIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN_DEFINE, STD_DEFINE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a020d-e4db-4de1-8dcb-c62b22b7c8f5",
   "metadata": {},
   "source": [
    "# np.full\n",
    "* 任意の値で全要素を初期化したndarrayを生成する\n",
    "* np.full(n, m, z) # n行 * m列 z次元\n",
    "\n",
    "# numpy.random.uniform(low, high, size)\n",
    "* 任意の範囲の連続一様分布から浮動小数点数の乱数を生成する\n",
    "\n",
    "# torch.unsqueeze\n",
    "* 指定した位置にサイズ1の次元を挿入する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1604a962-e990-4e5a-a65e-a2c7fb486ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_image(size=(400, 400, 3)):\n",
    "    initalize_image = PIL.Image.fromarray(np.uint8(np.full(size, 150)))\n",
    "    random_create_image = PIL.Image.fromarray(np.uint8(np.random.uniform(150, 180, size)))\n",
    "    image_tensor = normalise(random_create_image).unsqueeze(0)\n",
    "    image_np = image_tensor.numpy()\n",
    "    return random_create_image, image_tensor, image_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb57cf-4b3d-4803-b862-39b933125bc8",
   "metadata": {},
   "source": [
    "# PIL.Image.ANTIALIAS\n",
    "* アンチエイリアスで写真をキレイに縮小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "754e561c-553d-4e76-bb26-0910f5f5be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_path(path, resize=False, size=None):\n",
    "    image = PIL.Image.open(path)\n",
    "    \n",
    "    if size is not None:\n",
    "        image.thumbnailmb(size, PIL.Image.ANTIALIAS)\n",
    "        \n",
    "    if resize:\n",
    "        image_tensor = normalise_resize(image).unsqueeze(0)\n",
    "    else:\n",
    "        image_tensor = normalise(image).unsquee(0)\n",
    "    image_np = image_tensor.numpy()\n",
    "    \n",
    "    return image, image_tensor, image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "60cd3a6e-476a-4230-8b87-e21cb502a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    to_np = tensor.numpy()\n",
    "    mean = np.array(MEAN_DEFINE).reshape([1, 1, 3])\n",
    "    std = np.array(STD_DEFINE).reshape([1, 1, 3])\n",
    "    inp = to_np[0, :, :, :]\n",
    "    inp = inp.transpose(1, 2, 0)\n",
    "    inp = std * inp + mean\n",
    "    inp *= 255\n",
    "    inp = np.uint8(np.clip(inp, 0, 255))\n",
    "    return PIL.Image.fromarray(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3379d679-bb77-498a-bdae-fe960284f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_variable(image, requires_grad=False, cuda=False):\n",
    "    if cuda:\n",
    "        image = Variable(image.cuda(), requires_grad=requires_grad)\n",
    "    else:\n",
    "        image = Variable(image, requires_grad=requires_grad)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144642a-d544-4a7b-92a5-4c4ea25eb804",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "* Here we load a pretrained VGG-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "082069d3-6f72-4b3d-9845-52a32245330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16()\n",
    "# TODO: ここが不明\n",
    "# model.load_state_dict(torch.load('../input/vgg16/vgg16.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3da4c95d-f495-4c84-97a5-a5574ec7f0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "Not using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    USE_GPU = True\n",
    "\n",
    "print(model)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "if USE_GPU:\n",
    "    print('Using CUDA')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Not using CUDA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77f76e-76e5-42f2-a828-873b8ff58695",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* ここから疲れたので明日から理解します............."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aa747a14-3d11-4013-9d76-f15e43b8ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def octaver_fn(model, base_image, step_fn, octave_n=6, octave_scale=1.4, iter_n=10, **step_args):\n",
    "    octaves = [base_image]\n",
    "    \n",
    "    for i in range(octave_n - 1):\n",
    "        octaves.append(nd.zoom(octaves[-1], (1, 1, 1.0 / octave_scale, 1.0 / octave_scale), order=1))\n",
    "\n",
    "    detail = np.zeros_like(octaves[-1])\n",
    "    for octave, octave_base in enumerate(octaves[::-1]):\n",
    "        h, w = octave_base.shape[-2:]\n",
    "        \n",
    "        if octave > 0:\n",
    "            h1, w1 = detail.shape[-2:]\n",
    "            detail = nd.zoom(detail, (1, 1, 1.0 * h / h1, 1.0 * w / w1), order=1)\n",
    "        \n",
    "        src = octave_base + detail\n",
    "        \n",
    "        for i in range(iter_n):\n",
    "            src = step_fn(model, src, **step_args)\n",
    "\n",
    "        detail = src.numpy() - octave_base\n",
    "\n",
    "    return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251782-8cb4-4a34-8e2b-18dea8161d77",
   "metadata": {},
   "source": [
    "# フィルタの可視化\n",
    "* この関数は、レイヤー layer_index の filter_index のフィルタの活性度を最大化する画像を生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e30701ec-c49b-4329-9013-d119aaa46bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_step(model, img, layer_index, filter_index, step_size=5, display=True, use_L2=False):\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape([3, 1, 1])\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape([3, 1, 1])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    img_var = image_to_variable(torch.Tensor(img), requires_grad=True, cuda=USE_GPU)\n",
    "    optimizer = SGD([img_var], lr=step_size, weight_decay=1e-4)\n",
    "    \n",
    "    x = img_var\n",
    "    for index, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if index == layer_index:\n",
    "            break\n",
    "\n",
    "    output = x[0, filter_index]\n",
    "    loss = output.norm() #torch.mean(output)\n",
    "    loss.backward()\n",
    "    \n",
    "    if use_L2:\n",
    "        #L2 normalization on gradients\n",
    "        mean_square = torch.Tensor([torch.mean(img_var.grad.data ** 2) + 1e-5])\n",
    "        if USE_GPU:\n",
    "            mean_square = mean_square.cuda()\n",
    "        img_var.grad.data /= torch.sqrt(mean_square)\n",
    "        img_var.data.add_(img_var.grad.data * step_size)\n",
    "    else:\n",
    "        optimizer.step()\n",
    "    \n",
    "    result = img_var.data.cpu().numpy()\n",
    "    result[0, :, :, :] = np.clip(result[0, :, :, :], -mean / std, (1 - mean) / std)\n",
    "    \n",
    "    if display:\n",
    "        show_tensor(result)\n",
    "    \n",
    "    return torch.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9789a228-8916-4bf4-800a-ad962901a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filter(model, base_image, layer_index, filter_index, \n",
    "                     octave_n=6, octave_scale=1.4, iter_n=10, \n",
    "                     step_size=5, display=True, use_L2=False):\n",
    "    \n",
    "    return octaver_fn(\n",
    "                model, base_image, step_fn=filter_step, \n",
    "                octave_n=octave_n, octave_scale=octave_scale, \n",
    "                iter_n=iter_n, layer_index=layer_index, \n",
    "                filter_index=filter_index, step_size=step_size, \n",
    "                display=display, use_L2=use_L2\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cf153-f073-41e3-8e50-1019b28533b4",
   "metadata": {},
   "source": [
    "# 次に、与えられたレイヤーのフィルタの数を視覚化するヘルパー関数を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9a5f5d9e-0c89-49f9-bc4e-fd07fd3da132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_layer(layer_num, filter_start=10, filter_end=20, step_size=7, use_L2=False):\n",
    "    filters = []\n",
    "    titles = []\n",
    "    \n",
    "    _, _, img_np = initialize_image(size=(600, 600, 3))\n",
    "    for i in range(filter_start, filter_end):\n",
    "        title = \"Layer {} Filter {}\".format(layer_num , i)\n",
    "        print(title)\n",
    "        filter = visualize_filter(model, img_np, layer_num, filter_index=i, octave_n=2, iter_n=20, step_size=step_size, display=True, use_L2=use_L2)\n",
    "        filter_img = tensor_to_img(filter)\n",
    "        filter_img.save(title + \".jpg\")\n",
    "        filters.append(tensor_to_img(filter))\n",
    "        titles.append(title)\n",
    "        \n",
    "    \n",
    "    plot_images(filters, titles)\n",
    "    return filters, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "40014b0a-c607-443e-8d96-532863cb1680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 Filter 10\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'JPG'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images, titles \u001b[38;5;241m=\u001b[39m \u001b[43mshow_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_L2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[134], line 9\u001b[0m, in \u001b[0;36mshow_layer\u001b[0;34m(layer_num, filter_start, filter_end, step_size, use_L2)\u001b[0m\n\u001b[1;32m      7\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Filter \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(layer_num , i)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(title)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mvisualize_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctave_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_L2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_L2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m filter_img \u001b[38;5;241m=\u001b[39m tensor_to_img(\u001b[38;5;28mfilter\u001b[39m)\n\u001b[1;32m     11\u001b[0m filter_img\u001b[38;5;241m.\u001b[39msave(title \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[133], line 5\u001b[0m, in \u001b[0;36mvisualize_filter\u001b[0;34m(model, base_image, layer_index, filter_index, octave_n, octave_scale, iter_n, step_size, display, use_L2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_filter\u001b[39m(model, base_image, layer_index, filter_index, \n\u001b[1;32m      2\u001b[0m                      octave_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, octave_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.4\u001b[39m, iter_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      3\u001b[0m                      step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_L2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moctaver_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43moctave_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moctave_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctave_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moctave_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43miter_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miter_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilter_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_L2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_L2\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[131], line 18\u001b[0m, in \u001b[0;36moctaver_fn\u001b[0;34m(model, base_image, step_fn, octave_n, octave_scale, iter_n, **step_args)\u001b[0m\n\u001b[1;32m     15\u001b[0m     src \u001b[38;5;241m=\u001b[39m octave_base \u001b[38;5;241m+\u001b[39m detail\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iter_n):\n\u001b[0;32m---> 18\u001b[0m         src \u001b[38;5;241m=\u001b[39m \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     detail \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m-\u001b[39m octave_base\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src\n",
      "Cell \u001b[0;32mIn[132], line 35\u001b[0m, in \u001b[0;36mfilter_step\u001b[0;34m(model, img, layer_index, filter_index, step_size, display, use_L2)\u001b[0m\n\u001b[1;32m     32\u001b[0m result[\u001b[38;5;241m0\u001b[39m, :, :, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(result[\u001b[38;5;241m0\u001b[39m, :, :, :], \u001b[38;5;241m-\u001b[39mmean \u001b[38;5;241m/\u001b[39m std, (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m display:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mshow_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(result)\n",
      "Cell \u001b[0;32mIn[137], line 11\u001b[0m, in \u001b[0;36mshow_tensor\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m inp \u001b[38;5;241m=\u001b[39m std \u001b[38;5;241m*\u001b[39m inp \u001b[38;5;241m+\u001b[39m mean\n\u001b[1;32m      9\u001b[0m inp \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mshow_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[121], line 5\u001b[0m, in \u001b[0;36mshow_array\u001b[0;34m(image, format)\u001b[0m\n\u001b[1;32m      2\u001b[0m clip_np_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(np\u001b[38;5;241m.\u001b[39mclip(image, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m))\n\u001b[1;32m      3\u001b[0m buffer \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_np_image\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display(Image(data\u001b[38;5;241m=\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mgetvalue()))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/PIL/Image.py:2340\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2338\u001b[0m     save_handler \u001b[38;5;241m=\u001b[39m SAVE_ALL[\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mupper()]\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2340\u001b[0m     save_handler \u001b[38;5;241m=\u001b[39m \u001b[43mSAVE\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2342\u001b[0m created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'JPG'"
     ]
    }
   ],
   "source": [
    "images, titles = show_layer(1, use_L2=True, step_size=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
